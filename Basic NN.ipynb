{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essentials to implement a Neural Net using PyTorch\n",
    "\n",
    "From theory, we know that we need:\n",
    "- The Neural Net definition i.e.the input, the hidden layers that propagate the input and the output (a little bit more work for a RNN or more precise convolutional layers and pooling and flattening for a CNN)\n",
    "- An intialization of the weights or filters\n",
    "- A loss function and desired outputs\n",
    "\n",
    "To run our computation, we need to keep doing:\n",
    "- Push input after input forward along the net\n",
    "- Compute a loss\n",
    "- Propagate backwards and get gradients for each weight or filter\n",
    "- Alter the weights in direction opposite to gradient to decrease loss\n",
    "... until we have optimized weights enough that loss is within acceotable range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Neural Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This net looks like:  Net (\n",
      "  (hidden): Linear (8 -> 5)\n",
      "  (output): Linear (5 -> 2)\n",
      ")\n",
      "Classifying the inputs as if it is a trained model gives:  Variable containing:\n",
      "-0.5904 -0.8077\n",
      "-0.8998 -0.5220\n",
      "-0.6376 -0.7519\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Torch package\n",
    "import torch\n",
    "# Variable Class\n",
    "from torch.autograd import Variable\n",
    "# Neural Net Sub-package from Torch\n",
    "import torch.nn as nn\n",
    "# Functions from the Neural Net Sub-Package such as RELU\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# input_size will dictate size of first hidden layer\n",
    "# Here, have 3 samples with 8 coordinates each\n",
    "inputs = Variable(torch.randn(3,8), requires_grad=True)\n",
    "input_size = inputs.size()\n",
    "\n",
    "# Implement the Neural Net class from torch.nn.Module Class\n",
    "# Has all the useful stuff needed for a Neural Net\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Basic net architecture i.e. the layers needed in all\n",
    "        # How they are chained and non-linearity addition defined in forward()\n",
    "        # A layer is defined by number of inputs and outputs\n",
    "        # Input layer already specified by size of input fed when instantiating the net\n",
    "        self.hidden = nn.Linear(input_size[1], 5)\n",
    "        self.output = nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # How lyers link to each other is defined here\n",
    "        # Non-linearity added in between this chaining definition\n",
    "        \n",
    "        # Input 3 samples of size 8 into the hidden layer\n",
    "        x = self.hidden(inputs)\n",
    "        x = F.tanh(x)\n",
    "        x = self.output(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x\n",
    "\n",
    "    # Can also define any other helpers to be used by this NN here\n",
    "    \n",
    "# Instantiate Net architecture first\n",
    "net = Net()\n",
    "print 'This net looks like: ', net\n",
    "# Calling the net on a set of inputs does calls forward() i.e. forward props on all of them,\n",
    "# Useful if say you have a trained net and just need to classify test data\n",
    "result = net(inputs)\n",
    "print 'Classifying the inputs as if it is a trained model gives: ', result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Net\n",
    "So far we have just deined the Neural Net architecture, weights have been randomized as none have been specified and we saw how to forward propagate\n",
    "\n",
    "To train we still need:\n",
    "- Initializing weights\n",
    "- Specifying needed desired outputs for each input and... \n",
    "- A loss function to gauge how we far we are from those\n",
    "\n",
    "We need to proceed as follows:\n",
    "- Do full epochs through test data\n",
    "- Calculate the Loss and back propagate to minimize it\n",
    "- Repeat until loss satisfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss is now valued at: ', Variable containing:\n",
      " 1.3019\n",
      "[torch.FloatTensor of size 1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# In one epoch:\n",
    "#  The net forward propagates on the input\n",
    "#  Calculates loss using the loss function on outputs obtained and desired_output\n",
    "#  Optimizes the loss function using the optimizer\n",
    "#    That is, finds gradient of loss wrt. each weight (Recall the Variable Class that wraps around a Tensor)\n",
    "#    (SGD is an example seen in theory)\n",
    "#  Updates the weights based on those gradients\n",
    "\n",
    "def feed_forward_one_time(net, inputs, desired_outputs):\n",
    "    # Forward prop\n",
    "    output = net(inputs)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss_function = nn.MSELoss()\n",
    "    loss = loss_function(output, desired_outputs)\n",
    "    print('Loss is now valued at: ', loss)\n",
    "    return loss\n",
    "\n",
    "net = Net()\n",
    "loss = feed_forward_one_time(net, inputs, Variable(torch.rand(3, 2)))\n",
    "\n",
    "# Note: Both output and desired output ned to be Variables with same type of tensor inside\n",
    "# Cast a tensor by doing that_tensor_name.float() or .double() or .long() etc etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop and update weights (Optimization) in fine-grained manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer # 0 Parameters\n",
      "\n",
      " 0.0828 -0.0073  0.1013 -0.2768 -0.2450 -0.0431  0.1213 -0.2040\n",
      "-0.1974 -0.0602  0.0627 -0.1636 -0.3224  0.2514  0.1594 -0.1397\n",
      " 0.0430 -0.0236 -0.0016 -0.2356 -0.3284  0.2442 -0.1784  0.0559\n",
      "-0.3530  0.0957  0.1588 -0.3023 -0.2544  0.0558  0.0886  0.0960\n",
      " 0.1613  0.3233  0.1959  0.0641 -0.1029 -0.2467  0.2768  0.2288\n",
      "[torch.FloatTensor of size 5x8]\n",
      "\n",
      "layer # 0 Gradients\n",
      "\n",
      "-0.1148 -0.0310  0.0987  0.0060 -0.0584 -0.0197  0.0707 -0.0265\n",
      "-0.0634 -0.0547  0.2070  0.0677 -0.2211 -0.1692  0.2744  0.0019\n",
      " 0.0720  0.0282 -0.0797 -0.0147  0.0596  0.0341 -0.0716  0.0174\n",
      " 0.0032  0.0049 -0.0193 -0.0072  0.0222  0.0178 -0.0276 -0.0011\n",
      "-0.0013 -0.0002  0.0008 -0.0001 -0.0003  0.0002  0.0003 -0.0003\n",
      "[torch.FloatTensor of size 5x8]\n",
      "\n",
      "layer # 1 Parameters\n",
      "\n",
      "-0.3396\n",
      "-0.1061\n",
      " 0.2396\n",
      "-0.1187\n",
      "-0.1789\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "layer # 1 Gradients\n",
      "\n",
      "1.00000e-02 *\n",
      "  5.8034\n",
      "  1.2014\n",
      " -3.0349\n",
      "  0.0543\n",
      "  0.0761\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "layer # 2 Parameters\n",
      "\n",
      " 0.1815  0.3632 -0.3937 -0.3778 -0.2173\n",
      "-0.1770 -0.4433 -0.1216 -0.2963 -0.2220\n",
      "[torch.FloatTensor of size 2x5]\n",
      "\n",
      "layer # 2 Gradients\n",
      "\n",
      " 0.0666  0.1400 -0.0154  0.1523  0.0428\n",
      "-0.0666 -0.1400  0.0154 -0.1523 -0.0428\n",
      "[torch.FloatTensor of size 2x5]\n",
      "\n",
      "layer # 3 Parameters\n",
      "\n",
      " 0.2441\n",
      " 0.0208\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "layer # 3 Gradients\n",
      "\n",
      "1.00000e-02 *\n",
      "  8.8759\n",
      " -8.8759\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clear all gradient buffers for params to get fresh gradients\n",
    "net.zero_grad() \n",
    "# Backprop and get gradient for each and every param\n",
    "loss.backward()\n",
    "\n",
    "# Loss is a Variable that has its grad_fn spanning all the way back to inputs \n",
    "    # This allows Backpropagation wrt. every parameter\n",
    "    # Each parameter is in the net.parameters() generator:\n",
    "layer_count = 0\n",
    "for x in net.parameters():\n",
    "    print 'layer #', layer_count, 'Parameters'\n",
    "    print x.data\n",
    "    print 'layer #', layer_count, 'Gradients'\n",
    "    print x.grad.data\n",
    "    layer_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights from input layer to hidden BEFORE update:\n",
      "\n",
      " 0.0828 -0.0073  0.1013 -0.2768 -0.2450 -0.0431  0.1213 -0.2040\n",
      "-0.1974 -0.0602  0.0627 -0.1636 -0.3224  0.2514  0.1594 -0.1397\n",
      " 0.0430 -0.0236 -0.0016 -0.2356 -0.3284  0.2442 -0.1784  0.0559\n",
      "-0.3530  0.0957  0.1588 -0.3023 -0.2544  0.0558  0.0886  0.0960\n",
      " 0.1613  0.3233  0.1959  0.0641 -0.1029 -0.2467  0.2768  0.2288\n",
      "[torch.FloatTensor of size 5x8]\n",
      "\n",
      "weights from input layer to hidden AFTER update:\n",
      "\n",
      " 0.0839 -0.0069  0.1003 -0.2769 -0.2444 -0.0429  0.1206 -0.2037\n",
      "-0.1967 -0.0597  0.0607 -0.1643 -0.3202  0.2531  0.1566 -0.1397\n",
      " 0.0422 -0.0239 -0.0008 -0.2355 -0.3290  0.2439 -0.1777  0.0557\n",
      "-0.3531  0.0956  0.1590 -0.3022 -0.2546  0.0556  0.0889  0.0961\n",
      " 0.1613  0.3233  0.1958  0.0641 -0.1029 -0.2467  0.2768  0.2288\n",
      "[torch.FloatTensor of size 5x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now can update all those seen parameters using the gradients obtained\n",
    "def update_parameters(net, learning_rate = 0.01):\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "print 'weights from input layer to hidden BEFORE update:'\n",
    "print net.parameters().next().data\n",
    "update_parameters(net)\n",
    "print 'weights from input layer to hidden AFTER update:'\n",
    "print net.parameters().next().data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Easy built-in Optimization (only choose optimizer, loss function and step)\n",
    "One may want to optimize in the SGD way or any other way, without explicitly implementing the optimizer like we did (Just feedforward, calculate loss, backpropagate then let the optimizer do the rest with the gradients)\n",
    "\n",
    "Torch provides built-in optimizers e.g. SGD or Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight from input layer neurone 1 to hidden layer neurone 1 after update # 1 : \n",
      "-0.118415981531\n",
      "weight from input layer neurone 1 to hidden layer neurone 1 after update # 2 : \n",
      "-0.126368165016\n",
      "weight from input layer neurone 1 to hidden layer neurone 1 after update # 3 : \n",
      "-0.12321575731\n",
      "weight from input layer neurone 1 to hidden layer neurone 1 after update # 4 : \n",
      "-0.118741162121\n",
      "weight from input layer neurone 1 to hidden layer neurone 1 after update # 5 : \n",
      "-0.112103506923\n",
      "weight from input layer neurone 1 to hidden layer neurone 1 after update # 6 : \n",
      "-0.105015315115\n",
      "weight from input layer neurone 1 to hidden layer neurone 1 after update # 7 : \n",
      "-0.0995587557554\n",
      "Loss reached in the end:  0.896777868271\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "net = Net()\n",
    "# Choose loss function, optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "# Loop through:\n",
    "loss_value = 1000000\n",
    "update_count = 0\n",
    "while loss_value > 1:\n",
    "    # Regular feed-forward and loss calculation\n",
    "    output = net(inputs)\n",
    "    loss = loss_function(output, Variable(torch.rand(3, 2)))\n",
    "    loss_value = loss.data[0]\n",
    "    # Zero_grad on optimizer directly which now wraps around the net params\n",
    "    optimizer.zero_grad()\n",
    "    # Backpropagate\n",
    "    loss.backward()\n",
    "    # Adjust weights based on how this optimizer does it in theory\n",
    "    optimizer.step()\n",
    "    update_count += 1\n",
    "    print 'weight from input layer neurone 1 to hidden layer neurone 1 after update #', update_count, ': '\n",
    "    print net.parameters().next().data[0,0]\n",
    "    \n",
    "print 'Loss reached in the end: ', loss_value"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

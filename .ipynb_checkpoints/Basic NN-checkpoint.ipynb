{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essentials to implement a Neural Net using PyTorch\n",
    "\n",
    "From theory, we know that we need:\n",
    "- The Neural Net definition i.e.the input, the hidden layers that propagate the input and the output (a little bit more work for a RNN or more precise convolutional layers and pooling and flattening for a CNN)\n",
    "- An intialization of the weights or filters\n",
    "- A loss function and desired outputs\n",
    "\n",
    "To run our computation, we need to keep doing:\n",
    "- Push input after input forward along the net\n",
    "- Compute a loss\n",
    "- Propagate backwards and get gradients for each weight or filter\n",
    "- Alter the weights in direction opposite to gradient to decrease loss\n",
    "... until we have optimized weights enough that loss is within acceotable range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Neural Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This net looks like:  Net (\n",
      "  (hidden): Linear (8 -> 5)\n",
      "  (output): Linear (5 -> 2)\n",
      ")\n",
      "Classifying the inputs as if it is a trained model gives:  (Variable containing:\n",
      "-0.6546\n",
      "-0.6232\n",
      "-0.5901\n",
      "[torch.FloatTensor of size 3]\n",
      ", Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.LongTensor of size 3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Torch package\n",
    "import torch\n",
    "# Variable Class\n",
    "from torch.autograd import Variable\n",
    "# Neural Net Sub-package from Torch\n",
    "import torch.nn as nn\n",
    "# Functions from the Neural Net Sub-Package such as RELU\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# input_size will dictate size of first hidden layer\n",
    "# Here, have 3 samples with 8 coordinates each\n",
    "inputs = Variable(torch.randn(3,8), requires_grad=True)\n",
    "input_size = inputs.size()\n",
    "\n",
    "# Implement the Neural Net class from torch.nn.Module Class\n",
    "# Has all the useful stuff needed for a Neural Net\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Basic net architecture i.e. the layers needed in all\n",
    "        # How they are chained and non-linearity addition defined in forward()\n",
    "        # A layer is defined by number of inputs and outputs\n",
    "        # Input layer already specified by size of input fed when instantiating the net\n",
    "        self.hidden = nn.Linear(input_size[1], 5)\n",
    "        self.output = nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # How lyers link to each other is defined here\n",
    "        # Non-linearity added in between this chaining definition\n",
    "        \n",
    "        # Input 3 samples of size 8 into the hidden layer\n",
    "        x = self.hidden(inputs)\n",
    "        x = F.tanh(x)\n",
    "        x = self.output(x)\n",
    "        x = F.log_softmax(x)\n",
    "        predicted = torch.max(x, 1)\n",
    "        return predicted\n",
    "\n",
    "    # Can also define any other helpers to be used by this NN here\n",
    "    \n",
    "# Instantiate Net architecture first\n",
    "net = Net()\n",
    "print 'This net looks like: ', net\n",
    "# Calling the net on a set of inputs does calls forward() i.e. forward props on all of them,\n",
    "# Useful if say you have a trained net and just need to classify test data\n",
    "result = net(inputs)\n",
    "print 'Classifying the inputs as if it is a trained model gives: ', result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Net\n",
    "So far we have just deined the Neural Net architecture, weights have been randomized as none have been specified and we saw how to forward propagate\n",
    "\n",
    "To train we still need:\n",
    "- Initializing weights\n",
    "- Specifying needed desired outputs for each input and... \n",
    "- A loss function to gauge how we far we are from those\n",
    "\n",
    "We need to proceed as follows:\n",
    "- Do full epochs through test data\n",
    "- Calculate the Loss and back propagate to minimize it\n",
    "- Repeat until loss satisfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss is now valued at: ', Variable containing:\n",
      " 1.6667\n",
      "[torch.FloatTensor of size 1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# In one epoch:\n",
    "#  The net forward propagates on the input\n",
    "#  Calculates loss using the loss function on outputs obtained and desired_output\n",
    "#  Optimizes the loss function using the optimizer\n",
    "#    That is, finds gradient of loss wrt. each weight (Recall the Variable Class that wraps around a Tensor)\n",
    "#    (SGD is an example seen in theory)\n",
    "#  Updates the weights based on those gradients\n",
    "\n",
    "def feed_forward_one_time(net, inputs, desired_outputs):\n",
    "    # Forward prop\n",
    "    # Output from net is in the form of a list of:\n",
    "    #  1. actual logsoftmax value of biggest probability result\n",
    "    #  2. the biggest probability result\n",
    "    net_result = net(inputs)\n",
    "    output = Variable(net_result[1].data.float(), requires_grad=True)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss_function = nn.MSELoss()\n",
    "    loss = loss_function(output, desired_outputs)\n",
    "    print('Loss is now valued at: ', loss)\n",
    "\n",
    "net = Net()\n",
    "feed_forward_one_time(net, inputs, Variable(torch.arange(1, 4)))\n",
    "\n",
    "# Note: Both output and desired output ned to be Variables with same type of tensor inside\n",
    "# Cast a tensor by doing that_tensor_name.float() or .double() or .long() etc etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer # 0 Parameters\n",
      "Parameter containing:\n",
      "-0.3197 -0.3518  0.0701  0.0383  0.0509 -0.0275  0.2857  0.1449\n",
      "-0.1431  0.2345  0.2526 -0.2436  0.1383 -0.0528  0.1218  0.0756\n",
      "-0.1567 -0.2732  0.3497  0.2894  0.1846 -0.1207 -0.2182  0.2501\n",
      " 0.2786  0.0375  0.2465 -0.0741 -0.1640  0.0972  0.1692  0.3066\n",
      " 0.2091  0.0810 -0.2323  0.2969 -0.0808 -0.1355 -0.1512  0.3311\n",
      "[torch.FloatTensor of size 5x8]\n",
      "\n",
      "\n",
      "-0.3197 -0.3518  0.0701  0.0383  0.0509 -0.0275  0.2857  0.1449\n",
      "-0.1431  0.2345  0.2526 -0.2436  0.1383 -0.0528  0.1218  0.0756\n",
      "-0.1567 -0.2732  0.3497  0.2894  0.1846 -0.1207 -0.2182  0.2501\n",
      " 0.2786  0.0375  0.2465 -0.0741 -0.1640  0.0972  0.1692  0.3066\n",
      " 0.2091  0.0810 -0.2323  0.2969 -0.0808 -0.1355 -0.1512  0.3311\n",
      "[torch.FloatTensor of size 5x8]\n",
      "\n",
      "layer # 1 Parameters\n",
      "Parameter containing:\n",
      " 0.0632\n",
      " 0.2503\n",
      " 0.2324\n",
      "-0.1708\n",
      " 0.2452\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "\n",
      " 0.0632\n",
      " 0.2503\n",
      " 0.2324\n",
      "-0.1708\n",
      " 0.2452\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "layer # 2 Parameters\n",
      "Parameter containing:\n",
      "-0.1124  0.3807 -0.0022  0.2642 -0.2599\n",
      " 0.0564 -0.4358  0.2072  0.2914  0.1593\n",
      "[torch.FloatTensor of size 2x5]\n",
      "\n",
      "\n",
      "-0.1124  0.3807 -0.0022  0.2642 -0.2599\n",
      " 0.0564 -0.4358  0.2072  0.2914  0.1593\n",
      "[torch.FloatTensor of size 2x5]\n",
      "\n",
      "layer # 3 Parameters\n",
      "Parameter containing:\n",
      " 0.0367\n",
      " 0.1179\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "\n",
      " 0.0367\n",
      " 0.1179\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-0a30dd2b0cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Backprop and get gradient for each and every param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Loss is a Variable that has its grad_fn spanning all the way back to inputs \n",
    "    # This allows Backpropagation wrt. every parameter\n",
    "    # Each parameter is in the net.parameters() generator:\n",
    "layer_count = 0\n",
    "for x in net.parameters():\n",
    "    print 'layer #', layer_count, 'Parameters'\n",
    "    print x\n",
    "    layer_count += 1\n",
    "    print x.data\n",
    "        \n",
    "# Clear all gradient buffers for params to get fresh gradients\n",
    "net.zero_grad() \n",
    "# Backprop and get gradient for each and every param\n",
    "loss.backward()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
